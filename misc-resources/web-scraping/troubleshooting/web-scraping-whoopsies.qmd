---
title: "Web Scraping Whoopsies"
title-block-banner: true
author: "Manu Alcala + Jameson Carter"
date: "October 18, 2023"
format: 
    revealjs: 
        slide-number: c/t
        width: 1250
        height: 800
editor: visual
engine: knitr
---

## What is web scraping?

Broadly, it is the process of automatically pulling data from websites by reading their underlying code. Doing this gets complicated fast:

-   *Static* websites contain information in HTML code which **does not change** when you, the user, interact with it.
-   *Dynamic* websites have information in HTML code which **does change** as you interact with it.
-   *Static* vs. *Dynamic* websites require different code packages to scrape.
    -   For example, [selenium](https://www.selenium.dev/) is a common dynamic web scraping library and [scrapy](https://scrapy.org/) is a common static web scraping library.
    -   This is because a static scraper simply has to understand what it is looking for in the HTML. A dynamic scraper needs to simulate a human interacting with the site.

## When is it worth it?

-   Other data collection efforts are impossible or prone to human error
-   Getting new data releases quickly is valuable
-   Creating reproducible and reliable processes eases collaboration / quality assurance
-   Time spent coding \< time spent obtaining data in other ways + time spent on quality assurance

## What makes it hard? (can be multiple slides)

-   It is difficult to predict how much time a web scraping task will take.

-   Sites might change, introducing need to update.

-   Site maintainers may not be okay with data being scraped

-   Sites might be removed or stop being maintained.

# Example 1. Scraping state education sites

## Demonstrative example {video-loop="true"}

Slides describing florida example

# Example 2. Scraping Medicaid enrollment data

## Why Medicaid enrollment data?

Since Spring 2023, states have been disenrolling Medicaid beneficiaries who no longer qualify since the Public Health Emergency was ended.

::: {layout-ncol="1"}
![](images/kff-fig.png)
:::

## Why are the data interesting?

In anticipation of ["the great unwinding,"](https://www.brookings.edu/articles/medicaid-and-the-great-unwinding-a-high-stakes-implementation-challenge/) many states implemented policy changes to smooth the transition.

To understand the success of these policies, we wanted **time-series enrollment data for all 50 states**... from a Medicaid data system that is largely decentralized.

## Unreadable PDFs abound!

::: {layout-ncol="2"}
![An example from Louisiana](images/louisiana-horrible.png)

![and another from Ohio](images/ohio-horrible.png)
:::

## A sigh of relief...

Why page through PDFs when another organization's RAs can do it for you?

::: {layout-ncol="1"}
![](images/kff-page.png)
:::

## 1. Identify this is a scrape-able dynamic page

One URL with data you can only get by clicking each option!

::: {layout-ncol="1"}
![](images/kff-page-click.png)
:::

## 2. Confirm HTML actually contains the data

::: {layout-ncol="1"}
![](images/html-plot.png)
:::

## 3. Code for 30 hours! {#fun-slide background-video="images/web-surfer.mp4" background-video-loop="true" background-size="50px"}

```{css, echo=FALSE}
#fun-slide,
#fun-slide h2{
 color: blue;
 font-size: 200px;
 font-style: italic;
 font-family: cursive;
}

```

## 4. Bask in the glow of automated scraping

Whenever new data were released in the following 2 months, I re-ran [the code](https://github.com/UI-Research/web-scraping/blob/master/kff_unwinding.py) and got a well-formatted excel file as output.

::: {layout-ncol="1"}
![](images/example-scraped.png)
:::

## Little did I know, trouble was coming

::: {layout-ncol="1"}
![](images/trouble-in-paradise.png)
:::

## What happened?

2 months later, KFF **stopped updating** the dashboard and **changed how existing data was reported** on graphs.

::: {layout-ncol="1"}
![](images/broken-egg.png)
:::

# Concluding remarks

## Core questions to explore before you start

**Availability of data**

-   Are the data available through other routes?

-   Are the data produced by an organization that is invested in the problem long-term?

**Frequency of scraping**

-   Will I need to scrape the data multiple times?

-   What is the risk that the item scraped from the site will be changed?

**Time-value tradeoffs**

-   Is the time spent coding worth the payoff?

-   Will collecting data automatically save time on quality assurance?

# Questions?

The remainder of the time is reserved for group discussion!

# Thank you!

Please contact [Manu Alcala](mailto:malcalakovalski@urban.org) or [Jameson Carter](mailto:jamcarter@urban.org) if you would like to discuss either of these projects or scope whether a use-case is reasonable.
