---
title: "Web Scraping Whoopsies"
title-block-banner: true
author: "Manu Alcalá + Jameson Carter"
date: "October 18, 2023"
format: 
    revealjs: 
        slide-number: c/t
        width: 1250
        height: 800
        embed-resources: true
        incremental: true
execute:
  cache: false
editor: visual
engine: knitr
---

## What is web scraping?

Broadly, it is the process of automatically pulling data from websites by reading their underlying code. Doing this gets complicated fast:

-   *Static* websites contain information in HTML code which **does not change**
    when you, the user, interact with it.
-   *Dynamic* websites have information in HTML code which **does change** as
    you interact with it.
-   *Static* vs. *Dynamic* websites require different code packages to scrape.
    -   For example, [selenium](https://www.selenium.dev/) is a common dynamic
        web scraping library and [scrapy](https://scrapy.org/) is a common
        static web scraping library.
    -   This is because a static scraper simply has to understand what it is
        looking for in the HTML. A dynamic scraper needs to simulate a human
        interacting with the site.

::: notes
Generally, web scraping is the process of automatically reading the content of
websites, in our case to get data. Within a website's underlying code, there
might be something that you want to access. For example, a large table published
on a website with no download button. But getting that hypothetical table
depends on how the website is set up. Static sites are relatively easy to scrape
because all of the information is available to you through the code which built
the site, at all times. Dynamic sites work a little differently: they require
you, the user, to click on buttons and interact with the site.

Because of this, the tools we use differ. Static scrapers usually pull the
sites' code directly and then parse that code to extract relevant information.
Dynamic scrapers simulate a human being by clicking on the right series of
buttons, to trick the site into providing its data, which can then be parsed.
You will hear us mention both scrapy and selenium today. Selenium is used for
dynamic sites and scrapy is used for static sites .
:::

## When is it worth it?

-   Other data collection efforts are impossible or prone to human error
    -   Creating reproducible and reliable processes eases collaboration /
        quality assurance
-   Getting new data releases quickly is valuable
-   Time spent coding \< time spent obtaining data in other ways + time spent on
    quality assurance

::: notes
As you might have guessed, this takes some time. Before getting into it, you
should consider whether other methods of data collection are error-prone or so
daunting that you could never justify doing it by hand. You have emailed the
owner of the data to no avail and you fear that manually copying 1000 entries
could be subject to error (or a waste of time). Also, it is easier to
collaborate with code-based processes, explaining how you hand-generated tables
can sometimes feel like more trouble than its worth!

Additionally, consider whether getting new data quickly is valuable. For
example, if a government site releases data regularly you may want to have a
process ready to extract those data. Web scraping is great for this.

Finally, note that when planning these things it might seem like a bunch of code
on the front-end. But if the time spent coding is less than time spent obtaining
data in other ways PLUS time spent on quality assurance of those methods, it
could be well worth it! Validating processes is easier with code.
:::

## What makes it hard?

-   It is difficult to predict how much time a web scraping task will take.

-   Sites might change, introducing need to update.

    -   Sites might be removed, stop being maintained, or completely overhauled.

-   Site maintainers may not be okay with data being scraped. Quick plug for the
    TECH team's [Automated Data
    Guidelines](https://urbanorg.app.box.com/file/162920667208?s=mam9kpf48mu92f4ktpyuw218yf8j45w0).

::: notes
But it isn't always the right solution! It can be hard. First, it is difficult
to predict the time needed to complete a web scraping task. This is because
websites are often designed in different ways, however the more you do this the
easier it gets.

Sites change all the time! Which means web scraping code can be unstable.
Ideally, it would be better to access data through something like an API if the
organization offers one.

Finally, site maintainers might not be okay with their data being scraped.
Thankfully, Urban has some guidelines available to help insulate you from
repercussions. Next, I will dive into an example discussing some of our
trials/tribulations with scraping and then pass it off to Manu.
:::

# Example 1. Scraping Medicaid enrollment data

## Why Medicaid enrollment data?

Since Spring 2023, states have been dis-enrolling Medicaid beneficiaries who no
longer qualify since the Public Health Emergency was ended.

::: {layout-ncol="1"}
![](images/kff-fig.png)
:::

## Why are the data interesting?

In anticipation of ["the great
unwinding,"](https://www.brookings.edu/articles/medicaid-and-the-great-unwinding-a-high-stakes-implementation-challenge/)
many states implemented policy changes to smooth the transition.

To understand the success of these policies, we wanted **time-series enrollment
data for all 50 states**... from a Medicaid data system that is largely
decentralized.

## Unreadable PDFs abound!

::: {layout-ncol="2"}
![An example from Louisiana](images/louisiana-horrible.png)

![and another from Ohio](images/ohio-horrible.png)
:::

## A sigh of relief...

Why page through PDFs when another organization's RAs can do it for you?

::: {layout-ncol="1"}
![](images/kff-page.png)
:::

## 1. Identify this is a scrape-able dynamic page

One URL with data you can only get by clicking each option!

::: {layout-ncol="1"}
![](images/kff-page-click.png)
:::

## 2. Confirm HTML actually contains the data

::: {layout-ncol="1"}
![](images/html-plot.png)
:::

## 3. Code for 30 hours! {#fun-slide background-video="images/web-surfer.mp4" background-video-loop="true" background-size="50px"}

```{css, echo=FALSE}
#fun-slide,
#fun-slide h2{
 color: blue;
 font-size: 200px;
 font-style: italic;
 font-family: cursive;
}

```

## 4. Bask in the glow of automated scraping

Whenever new data were released in the following 2 months, I re-ran [the
code](https://github.com/UI-Research/web-scraping/blob/master/kff_unwinding.py)
and got a well-formatted excel file as output.

::: {layout-ncol="1"}
![](images/example-scraped.png)
:::

## Little did I know, trouble was coming

::: {layout-ncol="1"}
![](images/trouble-in-paradise.png)
:::

## What happened?

2 months later, KFF **stopped updating** the dashboard and **changed how
existing data was reported** on graphs.

::: {layout-ncol="1"}
![](images/broken-egg.png)
:::

# Example 2. Scraping Course Catalogs

::: notes

In the last example we saw that web scraping can be a very powerful tool for unlocking novel data sources that aren't otherwise readily available and to do so in an automated way. So now you're probably excited and thinking "this is so cool, the web is my oyster, I can get all the data". But, before you go on and try to scrape the entire web, I want to share a cautionary tale about the pitfalls of getting too carried away and going down a rabbit hole in larger scale projects.
:::

## Why course catalog data?

- States are increasingly interested work-based learning (WBL) as an important strategy for helping students prepare for and access good jobs, but measurement has been limited

- To understand the prevalence and types of WBL, we wanted **course-level data from community colleges across the country**


:::  notes

So for this particular project we wanted leverage web scraping to explore the universe of WBL opportunities, which is something states are very interested in expanding and consists of internships, apprenticeships, and practicums that focus on career preparation and training in supervised, work-like settings.

Community colleges have a long history of implementing WBL, but measurement has been virtually non-existent. We wanted to offer a better understanding of the prevalence and types of WBL by collecting course-level data from community colleges across the country and searching for keywords associated with WBL.

:::

## Not all catalogs are the same

::: {layout-ncol="2"}
![An example of course descriptions listed under department
pages](images/bc-course-descriptions.png)

![And an example containing links to course catalogs in .pdf
format](images/nfc-course-descriptions.png)
:::

::: notes
As you might imagine this is quite challenging since there's no centralized source where you can find course descriptions. So we needed to scrape catalogs from over a thousand school websites, with no general pattern to how they are arranged or what format they're in. Although a lot of catalogs were static, targeting the HTML code for each one would've been tremendously burdensome and vulnerable to changes. To make things harder, we didn't even have links to these catalogs. 

So we wanted to develop a more general strategy that could navigate from a home url to a catalog and extract all relevant text
:::

## Web crawling adventures

- **Scrapy** is a web scraping and web crawling framework to extract structured data from websites
- It uses *spiders*, which are self-contained web crawlers that follow links according to your instructions 


::: notes

Given the complexity of our problem, we needed a really powerful tool that could scale well. I got really excited about Scrapy since it's a framework designed specifically for scraping data at scale. In addition to extracting and storing web data, it uses "Spiders" which are self-contained web crawlers that can follow links and navigate through websites according to rules you specify, making it easier to scrape a large amount of data. 

In addition, Scrapy offers game changing features for more complex projects where you might need to manage cookies, write to a database, be careful about not overwhelming servers, and respecting mantainers rules for what data you can scrape.

So I decided it was worth learning and wanted to create a crawler that could start from a home page and follow links that contained keywords related to course catalogs. I powered through Scrapy's steep learning curve but after much pain and confusion, I finally got my spider to work on a small sample of schools!

:::

## Scrapy go brrrrr... {#scrapy-run background-video="images/scrapy-going-brr.mov" background-video-loop="true" background-size="50px"}


```{css, echo=FALSE}

#scrapy-run h2{
 color: #fdbf11;
 font-size: 200px;
}

```

::: notes


Our initial results looked promising. We were able to get helpful metadata and parse text from a few catalogs extremely quickly. So we decided we were ready to run this on our full sample
:::


## Whoopsie...

::: {layout-ncol="1"}
![](images/scrapy-whoops.png)
:::

::: notes

We were not ready. After a lot of hard work and many many hours refining this approach, our data was a hot mess: 
Our spider failed to collect **any** data for around 1/3 of schools, which wasn't great but given that no data on WBL exists, it still could've been useful if the remaining data were usable. But alas, most of our data had all sorts of problems that made us nervous about using it. 

Often multiple courses are listed on a page and we couldn't single them out courses without directly targeting the HTML. Our data also contained text from irrelevent links and we weren't able to develop rules that would correctly identify courses consistently. Another major problem was that we couldn't even verify whether we had found all course descriptions for a particular school.

So evidently, there were many many challenges with scraping at this scale that we didn't consider in advance and we decided to narrow our focus to a single state instead. 
:::

## A new direction
::: {.layout-ncol="1}
![](images/selenium.gif)
:::

::: notes

For our narrowed focus, we chose to analyze Florida because the Florida Department of Education (FLDOE) provides a database of all postsecondary courses offerings, which was exactly the information we were looking for on a smaller scale.

Although the information was public facing and readily accessible via a click-based interface, there was no option to download the text data we needed about each course in one batch.


:::

## Selenium and BeautifulSoup to the rescue!


::: {.layout-ncol="1"}
![](images/course-descriptions-data.png)
:::

::: notes
This was amazing because scraping a single web page, even a dynamic one, isn't nearly as challenging. We used Selenium to simulate the human navigation until arriving at a static page with course-level information. And finally we  used  BeautifulSoup to parse the specific HTML we cared about.

In the end, our research was still useful since we were able to accurately captured the universe of WBL in Florida by leveraging a centralized source we were confident in.  

:::

# Concluding remarks

::: notes
So in the end, trying to get data at a national scale was quite a rabbit hole and we should've asked several important questions before starting
:::
## Core questions to explore before you start

**Availability of data**

-   Are the data available through other routes?

-   Are the data produced by an organization that is invested in the problem
    long-term?

**Frequency of scraping**

-   Will I need to scrape the data multiple times?

-   What is the risk that the item scraped from the site will be changed?

**Time-value tradeoffs**

-   Is the time spent coding worth the payoff?

-   Will collecting data automatically save time on quality assurance?

::: notes
So web scraping is a really powerful skill that can be useful for research but you should ask yourself some important questions before you go down a rabbit hole

- Are there easier ways to access your data?
- How confident are you that the organization hosting the data is invested in mantaining it?

- If it's a one-off it might be overkill unless it's the only way
- How consequential would a breaking change to the website you're scraping for your application?

- And finally, make sure you know that the time spent working on your code is worth it before you drive yourself crazy but also consider efficiency gains from automating data collection in terms of quality assurance and reproducibility. 
:::
# Questions?

The remainder of the time is reserved for group discussion!



# Thank you!

Please contact [Manu Alcalá](mailto:malcalakovalski@urban.org) or [Jameson
Carter](mailto:jamcarter@urban.org) if you would like to discuss either of these
projects or scope whether a use-case is reasonable.
