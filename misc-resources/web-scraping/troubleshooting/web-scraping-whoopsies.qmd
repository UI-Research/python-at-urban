---
title: "Web Scraping Whoopsies"
title-block-banner: true
author: "Manu Alcalá + Jameson Carter"
date: "October 18, 2023"
format: 
    revealjs: 
        slide-number: c/t
        width: 1250
        height: 800
        embed-resources: true
execute:
  cache: false
editor: visual
engine: knitr
---

## What is web scraping?

Broadly, it is the process of automatically pulling data from websites by
reading their underlying code. Doing this gets complicated fast:

-   *Static* websites contain information in HTML code which **does not change**
    when you, the user, interact with it.
-   *Dynamic* websites have information in HTML code which **does change** as
    you interact with it.
-   *Static* vs. *Dynamic* websites require different code packages to scrape.
    -   For example, [selenium](https://www.selenium.dev/) is a common dynamic
        web scraping library and [scrapy](https://scrapy.org/) is a common
        static web scraping library.
    -   This is because a static scraper simply has to understand what it is
        looking for in the HTML. A dynamic scraper needs to simulate a human
        interacting with the site.

## When is it worth it?

-   Other data collection efforts are impossible or prone to human error
-   Getting new data releases quickly is valuable
-   Creating reproducible and reliable processes eases collaboration / quality
    assurance
-   Time spent coding \< time spent obtaining data in other ways + time spent on
    quality assurance

## What makes it hard? (can be multiple slides)

-   It is difficult to predict how much time a web scraping task will take.

-   Sites might change, introducing need to update.

-   Site maintainers may not be okay with data being scraped

-   Sites might be removed or stop being maintained.

# Example 1. Scraping Medicaid enrollment data

## Why Medicaid enrollment data?

Since Spring 2023, states have been disenrolling Medicaid beneficiaries who no
longer qualify since the Public Health Emergency was ended.

::: {layout-ncol="1"}
![](images/kff-fig.png)
:::

## Why are the data interesting?

In anticipation of ["the great
unwinding,"](https://www.brookings.edu/articles/medicaid-and-the-great-unwinding-a-high-stakes-implementation-challenge/)
many states implemented policy changes to smooth the transition.

To understand the success of these policies, we wanted **time-series enrollment
data for all 50 states**... from a Medicaid data system that is largely
decentralized.

## Unreadable PDFs abound!

::: {layout-ncol="2"}
![An example from Louisiana](images/louisiana-horrible.png)

![and another from Ohio](images/ohio-horrible.png)
:::

## A sigh of relief...

Why page through PDFs when another organization's RAs can do it for you?

::: {layout-ncol="1"}
![](images/kff-page.png)
:::

## 1. Identify this is a scrape-able dynamic page

One URL with data you can only get by clicking each option!

::: {layout-ncol="1"}
![](images/kff-page-click.png)
:::

## 2. Confirm HTML actually contains the data

::: {layout-ncol="1"}
![](images/html-plot.png)
:::

## 3. Code for 30 hours! {#fun-slide background-video="images/web-surfer.mp4" background-video-loop="true" background-size="50px"}

```{css, echo=FALSE}
#fun-slide,
#fun-slide h2{
 color: blue;
 font-size: 200px;
 font-style: italic;
 font-family: cursive;
}

```

## 4. Bask in the glow of automated scraping

Whenever new data were released in the following 2 months, I re-ran [the
code](https://github.com/UI-Research/web-scraping/blob/master/kff_unwinding.py)
and got a well-formatted excel file as output.

::: {layout-ncol="1"}
![](images/example-scraped.png)
:::

## Little did I know, trouble was coming

::: {layout-ncol="1"}
![](images/trouble-in-paradise.png)
:::

## What happened?

2 months later, KFF **stopped updating** the dashboard and **changed how
existing data was reported** on graphs.

::: {layout-ncol="1"}
![](images/broken-egg.png)
:::

# Example 2. Scraping Course Descriptions

## Work Based Learning (WBL)

Work-based learning can include internships, clinicals, co-ops and other opportunities to gain experience in a work setting. 

They are especially helpful to community college students

We sought to create a national dataset of WBL prevalance by scraping course descriptions 

## Course description data

- No centralized source, need to scrape each school individually
- No general pattern to where course descriptions can be found, how they are arranged, or what format they're in


::: {layout-ncol="2"}
![An example of course descriptions listed under department pages](images/bc-course-descriptions.png)

![And an example containing links to course catalogs in .pdf format](images/nfc-course-descriptions.png)
:::

::: {.notes}

:::

## Are we doomed?

- How do we find all the relevant links?
- How can we target HTML code when each website has an entirely different one?

## Web Crawling with Scrapy

- **Scrapy** is a web scraping framework designed for large-scale web scraping projects 
- It excels particularly in **web crawling**, i.e. traversing links and navigating websites in accordance with specified rules.
- It has several helpful features to respect scraping restrictions, avoid overwhelming servers, and pre-process scraped data

## Initial Web Scraping Attempt
Starting from a homepage, we used Scrapy to follow links containing keywords like “catalog” or “course descriptions" 

For each link, we scraped basic metadata and all the text present 

::: {layout-ncol="1"}
![Example output for one college](images/scrapy-output.png)

:::

## Whoopsie... 

After a lot of hard work refining our approach, our data was a hot mess:

- We did not get **any** data from about 1/3 of the schools 
- We couldn't uniquely identify courses which made it impossible to filter out duplicates
- We couldn't verify whether we had found all course descriptions


:::{.notes}

- About 1/3 of the time, our crawler got stuck at a dynamic page, failed to find any courses, and moved on to the next college without scraping any data

- For one school, we "found" 1,000 examples of WBL but when we examined it's site realized it only offered 80 courses a semester. 
:::


## A different approach

- We decided to lower our ambition and hone-in on a single state instead

::: {.layout-ncol="1}
![](images/selenium.gif)
:::

:::{.notes}
- The scale of our research question was too large to contend with issues in both site navigation and text extraction.
- Instead, we decided to narrow our focus to just one state
- We chose Florida because it's DOE website  contains all postsecondary course offerings in a dynamic page

:::

## Disaster Averted

- We used **Selenium** to navigate to each course until finding a static page containing course descriptions.
- We used **BeautifulSoup** next to parse specific HTML tags (which were standard across sites)

::: {.layout-ncol="1"}
![](images/course-descriptions-data.png)
:::

:::{.notes}

Put bullets and table side by side
:::
## In defense of Scrapy

- Although Scrapy didn’t fit our use case, I highly recommend it as a fast, powerful tool when:
  - Websites follow a similar structure
  - The list of pages to be scraped is already known
  - There are a few websites to crawl through but a large number of pages to scrape (e.g., scraping reviews for different products)

# Concluding remarks

## Core questions to explore before you start

**Availability of data**

-   Are the data available through other routes?

-   Are the data produced by an organization that is invested in the problem
    long-term?

**Frequency of scraping**

-   Will I need to scrape the data multiple times?

-   What is the risk that the item scraped from the site will be changed?

**Time-value tradeoffs**

-   Is the time spent coding worth the payoff?

-   Will collecting data automatically save time on quality assurance?

# Questions?

The remainder of the time is reserved for group discussion!

# Thank you!

Please contact [Manu Alcalá](mailto:malcalakovalski@urban.org) or [Jameson Carter](mailto:jamcarter@urban.org) if
you would like to discuss either of these projects or scope whether a use-case
is reasonable.
